{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using DenseModule.identity in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample input through network to determine dimensions...\n",
      "Embedded shape: (500, 100)\n",
      "Conv1 shape: (498, 128)\n",
      "Pool1 shape: (249, 128)\n",
      "Flat shape: (31872, 1)\n",
      "Flattened output size: 31872\n",
      "Starting training...\n",
      "Epoch 1, Batch 10/782, Loss: 0.8273, Accuracy: 40.62\n",
      "Epoch 1, Batch 20/782, Loss: 0.7877, Accuracy: 43.75\n",
      "Epoch 1, Batch 30/782, Loss: 0.7995, Accuracy: 40.62\n"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "# Include necessary modules\n",
    "include(\"Conv1DModule.jl\")\n",
    "include(\"Pool1DModule.jl\")\n",
    "include(\"FlattenModule.jl\")\n",
    "include(\"DenseModule.jl\")\n",
    "include(\"EmbeddingModule.jl\")\n",
    "\n",
    "include(\"IMDbDataLoader.jl\")\n",
    "include(\"LossAndAccuracy.jl\")\n",
    "include(\"NetworkHandlers.jl\")\n",
    "\n",
    "using .Conv1DModule, .Pool1DModule, .IMDbDataLoader, .FlattenModule, .DenseModule, .EmbeddingModule\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_features, train_labels = IMDbDataLoader.load_data(:train)\n",
    "train_x, train_y = IMDbDataLoader.preprocess_data(train_features, train_labels; one_hot=true)\n",
    "\n",
    "# Load and preprocess test data\n",
    "test_features, test_labels = IMDbDataLoader.load_data(:test)\n",
    "test_x, test_y = IMDbDataLoader.preprocess_data(test_features, test_labels; one_hot=true)\n",
    "\n",
    "# Create batches\n",
    "batch_size = 32  # Typically smaller for text data due to variable sequence lengths\n",
    "train_data = IMDbDataLoader.batch_data((train_x, train_y), batch_size; shuffle=true)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "\n",
    "# Initialize layers for text classification\n",
    "embedding_layer = EmbeddingModule.init_embedding_layer(vocab_size, embedding_dim, 123456789)\n",
    "conv_layer1 = Conv1DModule.init_conv1d_layer(3, embedding_dim, 128, 1, 0, 3697631579)\n",
    "pool_layer1 = Pool1DModule.init_pool1d_layer(2, 2)\n",
    "flatten_layer = FlattenModule.FlattenLayer()\n",
    "\n",
    "# Calculate the expected output size after convolutions and pooling\n",
    "# Use a sample input to determine dimensions dynamically\n",
    "println(\"Running sample input through network to determine dimensions...\")\n",
    "sample_input = train_x[1]\n",
    "sample_embedded = embedding_layer(sample_input)\n",
    "println(\"Embedded shape: \", size(sample_embedded))\n",
    "sample_conv1 = conv_layer1(sample_embedded)\n",
    "println(\"Conv1 shape: \", size(sample_conv1))\n",
    "sample_pool1 = pool_layer1(sample_conv1)\n",
    "println(\"Pool1 shape: \", size(sample_pool1))\n",
    "sample_flat = flatten_layer(sample_pool1)\n",
    "println(\"Flat shape: \", size(sample_flat))\n",
    "\n",
    "# Get the actual flattened size\n",
    "flattened_size = size(sample_flat, 1)\n",
    "println(\"Flattened output size: \", flattened_size)\n",
    "\n",
    "# Initialize dense layers with the correct input dimension\n",
    "dense_layer1 = DenseModule.init_dense_layer(flattened_size, 64, DenseModule.relu, DenseModule.relu_grad, 4172219205)\n",
    "dense_layer2 = DenseModule.init_dense_layer(64, 2, DenseModule.sigmoid, DenseModule.sigmoid_grad, 3762133366)\n",
    "\n",
    "# Assemble the network\n",
    "network = (embedding_layer, conv_layer1, pool_layer1, flatten_layer, dense_layer1, dense_layer2)\n",
    "\n",
    "# The backward_pass_master function\n",
    "function backward_pass_master(network, grad_loss)\n",
    "    for layer in reverse(network)\n",
    "        if isa(layer, EmbeddingModule.EmbeddingLayer)\n",
    "            grad_loss = EmbeddingModule.backward_pass(layer, grad_loss)\n",
    "        elseif isa(layer, Conv1DModule.Conv1DLayer)\n",
    "            grad_loss = Conv1DModule.backward_pass(layer, grad_loss)\n",
    "        elseif isa(layer, Pool1DModule.MaxPool1DLayer)\n",
    "            grad_loss = Pool1DModule.backward_pass(layer, grad_loss)\n",
    "        elseif isa(layer, DenseModule.DenseLayer)\n",
    "            grad_loss = DenseModule.backward_pass(layer, grad_loss)\n",
    "        elseif isa(layer, FlattenModule.FlattenLayer)\n",
    "            grad_loss = FlattenModule.backward_pass(layer, grad_loss)\n",
    "        else\n",
    "            println(\"No backward pass defined for layer type $(typeof(layer))\")\n",
    "        end\n",
    "    end\n",
    "    return grad_loss\n",
    "end\n",
    "\n",
    "# Update weights function\n",
    "function update_weights(network, learning_rate)\n",
    "    for layer in reverse(network)\n",
    "        if isa(layer, DenseModule.DenseLayer) || \n",
    "           isa(layer, Conv1DModule.Conv1DLayer) ||\n",
    "           isa(layer, EmbeddingModule.EmbeddingLayer)\n",
    "            \n",
    "            layer.grad_weights ./= batch_size\n",
    "            layer.grad_biases ./= batch_size\n",
    "            \n",
    "            layer.weights .-= learning_rate * layer.grad_weights\n",
    "            layer.biases .-= learning_rate * layer.grad_biases\n",
    "            \n",
    "            fill!(layer.grad_weights, 0)\n",
    "            fill!(layer.grad_biases, 0)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function to evaluate the model\n",
    "function evaluate_model(network, test_x, test_y)\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    # Limit to 100 samples for faster evaluation\n",
    "    num_samples = min(length(test_x), 100)\n",
    "\n",
    "    for i in 1:num_samples\n",
    "        input = test_x[i]\n",
    "        target = test_y[:, i]\n",
    "\n",
    "        # Forward pass\n",
    "        output = NetworkHandlers.forward_pass_master(network, input)\n",
    "\n",
    "        # Calculate loss and accuracy\n",
    "        loss, accuracy, _ = LossAndAccuracy.loss_and_accuracy(output, target)\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "    end\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / num_samples\n",
    "    avg_accuracy = total_accuracy / num_samples\n",
    "    return avg_loss, avg_accuracy\n",
    "end\n",
    "\n",
    "using .NetworkHandlers, .LossAndAccuracy\n",
    "epochs = 2  # Reduced for testing\n",
    "training_step = 0.001  # Smaller learning rate for stability\n",
    "\n",
    "println(\"Starting training...\")\n",
    "\n",
    "plot_loss = Float64[]\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    accumulated_accuracy_epoch = 0.0\n",
    "    samples_processed = 0\n",
    "    \n",
    "    for (batch_idx, batch) in enumerate(train_data)\n",
    "        batch_inputs, batch_targets = batch\n",
    "        batch_loss = 0.0\n",
    "        batch_accuracy = 0.0\n",
    "        \n",
    "        for j in 1:length(batch_inputs)\n",
    "            # Process each sequence in the batch\n",
    "            input = batch_inputs[j]\n",
    "            target = batch_targets[:, j]\n",
    "            \n",
    "            output = NetworkHandlers.forward_pass_master(network, input)\n",
    "            \n",
    "            loss, accuracy, grad_loss = LossAndAccuracy.loss_and_accuracy(output, target)\n",
    "            accumulated_accuracy_epoch += accuracy\n",
    "            batch_accuracy += accuracy\n",
    "            batch_loss += loss\n",
    "            \n",
    "            backward_pass_master(network, grad_loss)\n",
    "        end\n",
    "        \n",
    "        samples_processed += length(batch_inputs)\n",
    "        \n",
    "        # Average batch metrics and update weights\n",
    "        batch_loss /= length(batch_inputs)\n",
    "        push!(plot_loss, batch_loss)\n",
    "        \n",
    "        # Update weights after processing the batch\n",
    "        update_weights(network, training_step)\n",
    "        \n",
    "        # Print progress periodically\n",
    "        if batch_idx % 10 == 0\n",
    "            println(\"Epoch $(epoch), Batch $(batch_idx)/$(length(train_data)), Loss: $(round(batch_loss, digits=4)), Accuracy: $(round(batch_accuracy/length(batch_inputs), digits=2))\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Evaluate after each epoch\n",
    "    test_loss, test_accuracy = evaluate_model(network, test_x, test_y)\n",
    "    println(\"Epoch $(epoch) completed - Training Accuracy: $(round(accumulated_accuracy_epoch / samples_processed, digits=2)), Test Accuracy: $(round(test_accuracy, digits=2))\")\n",
    "end\n",
    "\n",
    "# Plot loss curve\n",
    "using Plots\n",
    "plot(plot_loss, xlabel=\"Batch\", ylabel=\"Loss\", title=\"Loss over batches\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
