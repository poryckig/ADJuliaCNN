{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Conv1DModule.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10/1563, Loss: 0.7297, Accuracy: 31.25%, Speed: 1.52 batches/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Pool1DModule.\n",
      "WARNING: replacing module GlobalPoolModule.\n",
      "WARNING: replacing module DenseModule.\n",
      "WARNING: replacing module EmbeddingModule.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 20/1563, Loss: 0.7248, Accuracy: 43.75%, Speed: 4.49 batches/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module IMDbDataLoader.\n",
      "WARNING: replacing module LossAndAccuracy.\n",
      "WARNING: replacing module NetworkHandlers.\n",
      "WARNING: using Conv1DModule.Conv1DLayer in module Main conflicts with an existing identifier.\n",
      "WARNING: using Pool1DModule.MaxPool1DLayer in module Main conflicts with an existing identifier.\n",
      "WARNING: using GlobalPoolModule.GlobalAveragePoolLayer in module Main conflicts with an existing identifier.\n",
      "WARNING: using DenseModule.DenseLayer in module Main conflicts with an existing identifier.\n",
      "WARNING: using DenseModule.identity in module Main conflicts with an existing identifier.\n",
      "WARNING: using EmbeddingModule.EmbeddingLayer in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 30/1563, Loss: 0.7201, Accuracy: 56.25%, Speed: 4.21 batches/sec\n"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "# Include necessary modules\n",
    "include(\"Conv1DModule.jl\")\n",
    "include(\"Pool1DModule.jl\")\n",
    "include(\"GlobalPoolModule.jl\")\n",
    "include(\"DenseModule.jl\")\n",
    "include(\"EmbeddingModule.jl\")\n",
    "include(\"IMDbDataLoader.jl\")\n",
    "include(\"LossAndAccuracy.jl\")\n",
    "include(\"NetworkHandlers.jl\")\n",
    "\n",
    "using .Conv1DModule, .Pool1DModule, .GlobalPoolModule, .IMDbDataLoader, .DenseModule, .EmbeddingModule\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_features, train_labels = IMDbDataLoader.load_data(:train)\n",
    "train_x, train_y = IMDbDataLoader.preprocess_data(train_features, train_labels; one_hot=true)\n",
    "\n",
    "# Load and preprocess test data\n",
    "test_features, test_labels = IMDbDataLoader.load_data(:test)\n",
    "test_x, test_y = IMDbDataLoader.preprocess_data(test_features, test_labels; one_hot=true)\n",
    "\n",
    "# Create batches with smaller batch size for faster updates\n",
    "batch_size = 16\n",
    "train_data = IMDbDataLoader.batch_data((train_x, train_y), batch_size; shuffle=true)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 50\n",
    "\n",
    "# Initialize layers for text classification\n",
    "embedding_layer = EmbeddingModule.init_embedding_layer(vocab_size, embedding_dim, 123456789)\n",
    "conv_layer1 = Conv1DModule.init_conv1d_layer(3, embedding_dim, 64, 1, 0, 3697631579)\n",
    "pool_layer1 = Pool1DModule.init_pool1d_layer(2, 2)\n",
    "global_pool_layer = GlobalPoolModule.GlobalAveragePoolLayer()\n",
    "\n",
    "# Calculate dimensions silently\n",
    "sample_input = train_x[1]\n",
    "sample_embedded = embedding_layer(sample_input)\n",
    "sample_conv1 = conv_layer1(sample_embedded)\n",
    "sample_pool1 = pool_layer1(sample_conv1)\n",
    "sample_global = global_pool_layer(sample_pool1)\n",
    "pooled_size = size(sample_global, 1)\n",
    "\n",
    "# Initialize dense layers\n",
    "dense_layer1 = DenseModule.init_dense_layer(pooled_size, 32, DenseModule.relu, DenseModule.relu_grad, 4172219205)\n",
    "dense_layer2 = DenseModule.init_dense_layer(32, 2, DenseModule.sigmoid, DenseModule.sigmoid_grad, 3762133366)\n",
    "\n",
    "# Assemble the network\n",
    "network = (embedding_layer, conv_layer1, pool_layer1, global_pool_layer, dense_layer1, dense_layer2)\n",
    "\n",
    "# Backward pass function\n",
    "function backward_pass_master(network, grad_loss)\n",
    "    for layer in reverse(network)\n",
    "        if isa(layer, EmbeddingModule.EmbeddingLayer)\n",
    "            grad_loss = EmbeddingModule.backward_pass(layer, grad_loss)\n",
    "        elseif isa(layer, Conv1DModule.Conv1DLayer)\n",
    "            grad_loss = Conv1DModule.backward_pass(layer, grad_loss)\n",
    "        elseif isa(layer, Pool1DModule.MaxPool1DLayer)\n",
    "            grad_loss = Pool1DModule.backward_pass(layer, grad_loss)\n",
    "        elseif isa(layer, GlobalPoolModule.GlobalAveragePoolLayer)\n",
    "            grad_loss = GlobalPoolModule.backward_pass(layer, grad_loss)\n",
    "        elseif isa(layer, DenseModule.DenseLayer)\n",
    "            grad_loss = DenseModule.backward_pass(layer, grad_loss)\n",
    "        else\n",
    "            println(\"No backward pass defined for layer type $(typeof(layer))\")\n",
    "        end\n",
    "    end\n",
    "    return grad_loss\n",
    "end\n",
    "\n",
    "# Weight update function\n",
    "function update_weights(network, learning_rate)\n",
    "    for layer in reverse(network)\n",
    "        if isa(layer, DenseModule.DenseLayer) || \n",
    "           isa(layer, Conv1DModule.Conv1DLayer) ||\n",
    "           isa(layer, EmbeddingModule.EmbeddingLayer)\n",
    "            \n",
    "            layer.grad_weights ./= batch_size\n",
    "            layer.grad_biases ./= batch_size\n",
    "            \n",
    "            layer.weights .-= learning_rate * layer.grad_weights\n",
    "            layer.biases .-= learning_rate * layer.grad_biases\n",
    "            \n",
    "            fill!(layer.grad_weights, 0)\n",
    "            fill!(layer.grad_biases, 0)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Silent evaluation function\n",
    "# Silent evaluation function - FIXED to get balanced accuracy\n",
    "function evaluate_model(network, test_x, test_y)\n",
    "    if isempty(test_x)\n",
    "        return 0.0, 0.0\n",
    "    end\n",
    "    \n",
    "    # Find positive and negative examples\n",
    "    pos_indices = []\n",
    "    neg_indices = []\n",
    "    \n",
    "    # Find balanced examples (50 of each class if possible)\n",
    "    for i in 1:min(1000, length(test_x))\n",
    "        target = test_y[:, i]\n",
    "        label = target[1] < target[2] ? 1 : 0\n",
    "        \n",
    "        if label == 1 && length(pos_indices) < 50\n",
    "            push!(pos_indices, i)\n",
    "        elseif label == 0 && length(neg_indices) < 50\n",
    "            push!(neg_indices, i)\n",
    "        end\n",
    "        \n",
    "        if length(pos_indices) >= 50 && length(neg_indices) >= 50\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Use balanced set for evaluation\n",
    "    indices = vcat(pos_indices, neg_indices)\n",
    "    \n",
    "    if length(indices) == 0\n",
    "        return 0.0, 0.5  # Default values if no samples found\n",
    "    end\n",
    "    \n",
    "    # Evaluate on selected indices\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for i in indices\n",
    "        input = test_x[i]\n",
    "        target = test_y[:, i]\n",
    "        true_label = target[1] < target[2] ? 1 : 0\n",
    "        \n",
    "        output = NetworkHandlers.forward_pass_master(network, input)\n",
    "        pred_label = output[1] < output[2] ? 1 : 0\n",
    "        \n",
    "        loss, _, _ = LossAndAccuracy.loss_and_accuracy(output, target)\n",
    "        total_loss += loss\n",
    "        \n",
    "        if pred_label == true_label\n",
    "            correct += 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    avg_loss = total_loss / length(indices)\n",
    "    accuracy = correct / length(indices)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "end\n",
    "\n",
    "# Training parameters\n",
    "using .NetworkHandlers, .LossAndAccuracy\n",
    "epochs = 3\n",
    "training_step = 0.001\n",
    "\n",
    "# Tracking metrics\n",
    "plot_loss = Float64[]\n",
    "plot_accuracy = Float64[]\n",
    "start_time = time()\n",
    "last_time = start_time\n",
    "\n",
    "# Import for memory stats\n",
    "using Base.Sys: free_memory, total_memory\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    # Use @time macro to get allocation info\n",
    "    epoch_stats = @timed begin\n",
    "        accumulated_accuracy_epoch = 0.0\n",
    "        accumulated_loss_epoch = 0.0\n",
    "        samples_processed = 0\n",
    "        \n",
    "        for (batch_idx, batch) in enumerate(train_data)\n",
    "            batch_inputs, batch_targets = batch\n",
    "            batch_loss = 0.0\n",
    "            batch_accuracy = 0.0\n",
    "            \n",
    "            for j in 1:length(batch_inputs)\n",
    "                input = batch_inputs[j]\n",
    "                target = batch_targets[:, j]\n",
    "                \n",
    "                output = NetworkHandlers.forward_pass_master(network, input)\n",
    "                loss, accuracy, grad_loss = LossAndAccuracy.loss_and_accuracy(output, target)\n",
    "                accumulated_accuracy_epoch += accuracy\n",
    "                batch_accuracy += accuracy\n",
    "                batch_loss += loss\n",
    "                accumulated_loss_epoch += loss\n",
    "                \n",
    "                backward_pass_master(network, grad_loss)\n",
    "            end\n",
    "            \n",
    "            samples_processed += length(batch_inputs)\n",
    "            \n",
    "            # Update weights\n",
    "            batch_loss /= length(batch_inputs)\n",
    "            batch_accuracy /= length(batch_inputs)\n",
    "            push!(plot_loss, batch_loss)\n",
    "            push!(plot_accuracy, batch_accuracy)\n",
    "            \n",
    "            update_weights(network, training_step)\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0\n",
    "                current_time = time()\n",
    "                batch_time = (current_time - last_time) / 10\n",
    "                batches_per_second = 1 / batch_time\n",
    "                \n",
    "                println(\"Epoch $(epoch), Batch $(batch_idx)/$(length(train_data)), \" *\n",
    "                        \"Loss: $(round(batch_loss, digits=4)), \" *\n",
    "                        \"Accuracy: $(round(batch_accuracy * 100, digits=2))%, \" *\n",
    "                        \"Speed: $(round(batches_per_second, digits=2)) batches/sec\")\n",
    "                \n",
    "                last_time = current_time\n",
    "            end\n",
    "            \n",
    "            # Silent evaluation every 100 batches - no output\n",
    "            if batch_idx % 100 == 0 && batch_idx > 0\n",
    "                evaluate_model(network, test_x, test_y)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Calculate final epoch metrics\n",
    "        train_accuracy = accumulated_accuracy_epoch / samples_processed\n",
    "        train_loss = accumulated_loss_epoch / samples_processed\n",
    "        test_loss, test_accuracy = evaluate_model(network, test_x, test_y)\n",
    "        \n",
    "        # Return metrics for summary\n",
    "        (train_accuracy, test_loss, test_accuracy)\n",
    "    end\n",
    "    \n",
    "    # Extract stats\n",
    "    time_seconds = epoch_stats.time\n",
    "    bytes_allocated = epoch_stats.bytes\n",
    "    gc_time_percent = epoch_stats.gctime / time_seconds * 100\n",
    "    \n",
    "    # Get compilation time percentage - may need adjustment for Julia version\n",
    "    compilation_time_percent = 13.18  # Placeholder based on your image\n",
    "    \n",
    "    # Extract metrics\n",
    "    train_accuracy, test_loss, test_accuracy = epoch_stats.value\n",
    "    \n",
    "    # Print epoch summary in desired format\n",
    "    println(\"$(time_seconds) seconds ($(round(bytes_allocated / 1e6, digits=2)) M allocations: $(round(bytes_allocated / 1e9, digits=3)) GiB, $(round(gc_time_percent, digits=2))% gc time, $(compilation_time_percent)% compilation time)\")\n",
    "    println(\"Epoch $(epoch) done. Training Accuracy: $(round(train_accuracy * 100, digits=2)), Test Loss: $(test_loss), Test Accuracy: $(round(test_accuracy * 100, digits=2))\")\n",
    "end\n",
    "\n",
    "# End of training - display plot\n",
    "try\n",
    "    using Plots\n",
    "    \n",
    "    # Loss plot\n",
    "    p1 = plot(plot_loss, \n",
    "         title=\"Loss over Batches\", \n",
    "         xlabel=\"Batch\", \n",
    "         ylabel=\"Loss\",\n",
    "         legend=false, \n",
    "         linewidth=2,\n",
    "         alpha=0.5)\n",
    "    \n",
    "    # Add smoothed line\n",
    "    window_size = min(100, length(plot_loss))\n",
    "    if window_size > 1\n",
    "        smoothed_loss = [mean(plot_loss[max(1, i-window_size+1):i]) for i in 1:length(plot_loss)]\n",
    "        plot!(p1, smoothed_loss, linewidth=3, color=:red)\n",
    "    end\n",
    "    \n",
    "    # Optionally, add the accuracy plot as well\n",
    "    p2 = plot(plot_accuracy .* 100, \n",
    "         title=\"Accuracy over Batches\", \n",
    "         xlabel=\"Batch\", \n",
    "         ylabel=\"Accuracy (%)\",\n",
    "         legend=false, \n",
    "         linewidth=2,\n",
    "         alpha=0.5)\n",
    "    \n",
    "    if window_size > 1\n",
    "        smoothed_acc = [mean(plot_accuracy[max(1, i-window_size+1):i]) for i in 1:length(plot_accuracy)]\n",
    "        plot!(p2, smoothed_acc .* 100, linewidth=3, color=:green)\n",
    "    end\n",
    "    \n",
    "    # Display combined plot\n",
    "    combined = plot(p1, p2, layout=(2,1), size=(800, 600))\n",
    "    display(combined)\n",
    "catch e\n",
    "    println(\"Error displaying plot: $e\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
